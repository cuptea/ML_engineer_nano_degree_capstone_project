{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Recommendation System Web App\n",
    "\n",
    "_Deep Learning Nanodegree Program | Capstone Project_\n",
    "\n",
    "---\n",
    "\n",
    "## General Outline\n",
    "\n",
    "Recall the general outline for SageMaker projects using a notebook instance.\n",
    "\n",
    "1. Download or otherwise retrieve the data.\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "4. Train a chosen model.\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n",
    "\n",
    "**TODO**: provide more information on the special setup related to the project here.\n",
    "\n",
    "\n",
    "------\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Project Overview\n",
    "\n",
    "#### Background Info\n",
    "\n",
    "In this project, our goal will be to have a simple web page which one can use to select a song and a music app user. The web page will then send the data to our deployed model which will predict whether the song should be recommendated to the music app user.\n",
    "\n",
    "\n",
    "#### The Project Origin\n",
    "\n",
    "[WSDM - KKBox's Music Recommendation Challenge](https://www.kaggle.com/c/kkbox-music-recommendation-challenge/data)\n",
    "\n",
    "####  Related data sets\n",
    "\n",
    "We will be using the [kkbox dataset](https://www.kaggle.com/bvmadduluri/wsdm-kkbox) which has been used for the [WSDM - KKBox's Music Recommendation Challenge](https://www.kaggle.com/c/kkbox-music-recommendation-challenge/data) kaggle competetion.\n",
    "\n",
    "\n",
    "- **For detailed data preprocessing, please find the dedicated notebook within the same folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.16.63)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.14.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.4.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.8)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.3.4)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.63 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.19.63)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.63->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.63->boto3>=1.14.12->sagemaker==1.72.0) (1.26.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=386358 sha256=c47bc5392f1844a496857acf174acaa461b3e23c5e88a9f0e278ee695b78ca1a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/58/70/85faf4437568bfaa4c419937569ba1fe54d44c5db42406bbd7\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.24.1\n",
      "    Uninstalling sagemaker-2.24.1:\n",
      "      Successfully uninstalled sagemaker-2.24.1\n",
      "Successfully installed sagemaker-1.72.0 smdebug-rulesconfig-0.1.4\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Make sure that we use SageMaker 1.x\n",
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Problem Statement\n",
    "\n",
    "#### problem defintion:\n",
    "given the song id, the user id (msno) and many other meta information of song and user, the trained model can predict whehter the song can be recommended to the user.\n",
    "\n",
    "\n",
    "#### A strategy for solving the problem\n",
    "we will try to extend the collaborative filering with the rich meta information of song and user. After some investigation, the following paper can be used as the base of our solution. \n",
    "\n",
    "- Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.  Neural collaborative Filtering. WWW 2-17, 173–182. [Link](https://arxiv.org/pdf/1708.05031.pdf)\n",
    "\n",
    "We will modify the NCF model architecture to include additional meta information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "discussion of the expected solution\n",
    "\n",
    "\n",
    "### 1.3 Metrics:\n",
    "Loss function\n",
    "- binary cross entropy\n",
    "\n",
    "The preformance of the algorithm is measured by \n",
    "- precision, \n",
    "- recall, \n",
    "- f1 score\n",
    "- roc-auc score.\n",
    "\n",
    "**For model evaluation, please check out the visualization notebook based on local model evaluation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('./data/train_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>song_id</th>\n",
       "      <th>source_system_tab</th>\n",
       "      <th>source_screen_name</th>\n",
       "      <th>source_type</th>\n",
       "      <th>target</th>\n",
       "      <th>song_length</th>\n",
       "      <th>composer</th>\n",
       "      <th>lyricist</th>\n",
       "      <th>language</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>genre</th>\n",
       "      <th>artist</th>\n",
       "      <th>city</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>app_age</th>\n",
       "      <th>registration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27736</td>\n",
       "      <td>2177998</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>206471</td>\n",
       "      <td>241316</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>200</td>\n",
       "      <td>58</td>\n",
       "      <td>131519</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2103</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27736</td>\n",
       "      <td>456984</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>187802</td>\n",
       "      <td>147477</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>153</td>\n",
       "      <td>103</td>\n",
       "      <td>196188</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2103</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27736</td>\n",
       "      <td>2193373</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>247803</td>\n",
       "      <td>35078</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "      <td>26332</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2103</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27736</td>\n",
       "      <td>1821216</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>181115</td>\n",
       "      <td>117840</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>161</td>\n",
       "      <td>103</td>\n",
       "      <td>156774</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2103</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27736</td>\n",
       "      <td>2283904</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>257369</td>\n",
       "      <td>132191</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>161</td>\n",
       "      <td>89</td>\n",
       "      <td>144996</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2103</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1903</td>\n",
       "      <td>1450857</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>239709</td>\n",
       "      <td>53578</td>\n",
       "      <td>75968</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>200</td>\n",
       "      <td>89</td>\n",
       "      <td>71037</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>396</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1903</td>\n",
       "      <td>197806</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>149629</td>\n",
       "      <td>64332</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>161</td>\n",
       "      <td>165</td>\n",
       "      <td>135485</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>396</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1903</td>\n",
       "      <td>1484475</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>226440</td>\n",
       "      <td>99175</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "      <td>81</td>\n",
       "      <td>165</td>\n",
       "      <td>71325</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>396</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1903</td>\n",
       "      <td>674</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>179908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>161</td>\n",
       "      <td>89</td>\n",
       "      <td>103736</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>396</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1903</td>\n",
       "      <td>46067</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>236472</td>\n",
       "      <td>64332</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>161</td>\n",
       "      <td>165</td>\n",
       "      <td>135485</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>396</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       msno  song_id  source_system_tab  source_screen_name  source_type  \\\n",
       "0     27736  2177998                  7                   5           11   \n",
       "1     27736   456984                  7                   5           11   \n",
       "2     27736  2193373                  7                   5           11   \n",
       "3     27736  1821216                  7                   5           11   \n",
       "4     27736  2283904                  7                   5           11   \n",
       "...     ...      ...                ...                 ...          ...   \n",
       "9995   1903  1450857                  6                  16           11   \n",
       "9996   1903   197806                  6                  16           11   \n",
       "9997   1903  1484475                  2                  20            7   \n",
       "9998   1903      674                  8                   4           12   \n",
       "9999   1903    46067                  2                  20            7   \n",
       "\n",
       "      target  song_length  composer  lyricist  language  year  country  genre  \\\n",
       "0          1       206471    241316         0         5  2016      200     58   \n",
       "1          1       187802    147477         0         5  2016      153    103   \n",
       "2          1       247803     35078         0         5  2016      161      5   \n",
       "3          1       181115    117840         0         5  2016      161    103   \n",
       "4          0       257369    132191         0         5  2013      161     89   \n",
       "...      ...          ...       ...       ...       ...   ...      ...    ...   \n",
       "9995       1       239709     53578     75968         5  2013      200     89   \n",
       "9996       0       149629     64332         0         5  2016      161    165   \n",
       "9997       0       226440     99175         0         7  2016       81    165   \n",
       "9998       0       179908         0         0         5  1999      161     89   \n",
       "9999       0       236472     64332         0         5  2016      161    165   \n",
       "\n",
       "      artist  city  gender  registered_via  app_age  registration  \n",
       "0     131519    14       1               5     2103          2838  \n",
       "1     196188    14       1               5     2103          2838  \n",
       "2      26332    14       1               5     2103          2838  \n",
       "3     156774    14       1               5     2103          2838  \n",
       "4     144996    14       1               5     2103          2838  \n",
       "...      ...   ...     ...             ...      ...           ...  \n",
       "9995   71037    14       1               5      396          4523  \n",
       "9996  135485    14       1               5      396          4523  \n",
       "9997   71325    14       1               5      396          4523  \n",
       "9998  103736    14       1               5      396          4523  \n",
       "9999  135485    14       1               5      396          4523  \n",
       "\n",
       "[10000 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload the data to S3\n",
    "\n",
    "We will need to upload the training dataset to S3 in order for our training code to access it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the training data\n",
    "\n",
    "\n",
    "Next, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/recommendation_system'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./upload/\"\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The cell above uploads the entire contents of our data directory. This includes the `word_dict.pkl` file. This is fortunate as we will need this later on when we create an endpoint that accepts an arbitrary review. For now, we will just take note of the fact that it resides in the data directory (and so also in the S3 training bucket) and that we will need to make sure it gets saved in the model directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and Train the PyTorch Model\n",
    "\n",
    "A model comprises three objects\n",
    "\n",
    " - Model Artifacts,\n",
    " - Training Code, and\n",
    " - Inference Code,\n",
    " \n",
    "We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we have provided the necessary model object in the `model.py` file, inside of the `code` folder. You can see the provided implementation by running the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering \n",
    "\n",
    "we have implementated teh Neural Collaborative Filtering (NCF) model based on paper:\n",
    "- Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.  Neural collaborative Filtering. WWW 2-17, 173–182.\n",
    "\n",
    "Link to the paper:\n",
    "- https://arxiv.org/pdf/1708.05031.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to(obj, device):\n",
    "    \"\"\"helpter function to move complex data structure to GPUs\"\"\"\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, dict):\n",
    "        res = {}\n",
    "        for k, v in obj.items():\n",
    "            res[k] = move_to(v, device)\n",
    "        return res\n",
    "    elif isinstance(obj, list):\n",
    "        res = []\n",
    "        for v in obj:\n",
    "            res.append(move_to(v, device))\n",
    "        return res\n",
    "    else:\n",
    "        raise TypeError(\"Invalid type for move_to\")\n",
    "        \n",
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch, batch['target'].float()\n",
    "            \n",
    "            batch_X = move_to(batch_X,device)\n",
    "            batch_y = move_to(batch_y,device)\n",
    "            \n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing we have the training method above, we will test that it is working by writing a bit of code in the notebook that executes our training method on the small sample training set that we loaded earlier. The reason for doing this in the notebook is so that we have an opportunity to fix any errors that arise early when they are easier to diagnose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class KKbox_set(Dataset):\n",
    "    def __init__(self, df, transformation=None):\n",
    "        super(KKbox_set).__init__()\n",
    "        self.data = df\n",
    "        self.data = list(self.data.T.to_dict().values())\n",
    "        self.transform = transformation     \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"return a role assignment in a dictionary form\"\"\"\n",
    "        sample = self.data[index]\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a `requirements.txt` file and install any required Python libraries, after which the training script will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Training the model\n",
    "\n",
    "When a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the `train` directory is a file called `train.py` which has been provided and which contains most of the necessary code to train our model. The only thing that is missing is the implementation of the `train()` method which you wrote earlier in this notebook.\n",
    "\n",
    "**TODO**: Copy the `train()` method written above and paste it into the `train/train.py` file where required.\n",
    "\n",
    "The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided `train/train.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"code\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'embedding_dim': 10,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28 19:53:26 Starting - Starting the training job...\n",
      "2021-02-28 19:53:28 Starting - Launching requested ML instances......\n",
      "2021-02-28 19:54:35 Starting - Preparing the instances for training......\n",
      "2021-02-28 19:55:50 Downloading - Downloading input data.........\n",
      "2021-02-28 19:57:09 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-02-28 19:57:32,174 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-02-28 19:57:32,198 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-02-28 19:57:35,231 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-02-28 19:57:35,473 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-02-28 19:57:35,474 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-02-28 19:57:35,474 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-02-28 19:57:35,474 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (1.15.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas->-r requirements.txt (line 1)) (1.11.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9_5sgqpt/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, pandas, train\u001b[0m\n",
      "\u001b[34mSuccessfully installed pandas-0.24.2 pytz-2021.1 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-02-28 19:57:41,046 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-670934899460/sagemaker-pytorch-2021-02-28-19-53-25-732/source/sourcedir.tar.gz\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-02-28-19-53-25-732\",\n",
      "    \"log_level\": 20,\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"embedding_dim\": 10\n",
      "    },\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"TrainingInputMode\": \"File\"\n",
      "        }\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"num_gpus\": 1,\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_HPS={\"embedding_dim\":10,\"epochs\":10}\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"embedding_dim\":10,\"epochs\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-02-28-19-53-25-732\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-670934899460/sagemaker-pytorch-2021-02-28-19-53-25-732/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--embedding_dim\",\"10\",\"--epochs\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-670934899460/sagemaker-pytorch-2021-02-28-19-53-25-732/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EMBEDDING_DIM=10\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --embedding_dim 10 --epochs 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\n",
      "    msno  song_id  source_system_tab  ...  registered_via  app_age  registration\u001b[0m\n",
      "\u001b[34m0  27736  2177998                  7  ...               5     2103          2838\u001b[0m\n",
      "\u001b[34m1  27736   456984                  7  ...               5     2103          2838\u001b[0m\n",
      "\u001b[34m2  27736  2193373                  7  ...               5     2103          2838\u001b[0m\n",
      "\u001b[34m3  27736  1821216                  7  ...               5     2103          2838\u001b[0m\n",
      "\u001b[34m4  27736  2283904                  7  ...               5     2103          2838\n",
      "\u001b[0m\n",
      "\u001b[34m[5 rows x 19 columns]\u001b[0m\n",
      "\n",
      "2021-02-28 19:57:31 Training - Training image download completed. Training in progress.\u001b[34mModel loaded with embedding_dim 10.\u001b[0m\n",
      "\u001b[34mEpoch: 1, BCELoss: 1.0684089958667755\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.9221465051174164\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.8781325310468674\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.8397553712129593\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.8169590711593628\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.7828522503376008\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.7444434642791748\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.6952005803585053\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.7426571398973465\u001b[0m\n",
      "\u001b[34mEpoch: 10, BCELoss: 0.6837542593479157\u001b[0m\n",
      "\u001b[34m2021-02-28 19:58:01,224 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-02-28 19:58:02 Uploading - Uploading generated training model\n",
      "2021-02-28 19:59:05 Completed - Training job completed\n",
      "Training seconds: 195\n",
      "Billable seconds: 195\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing the model\n",
    "\n",
    "As mentioned at the top of this notebook, we will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. We will do this so that we can make sure that the deployed model is working correctly.\n",
    "\n",
    "## Step 6: Deploy the model for testing\n",
    "\n",
    "Now that we have trained our model, we would like to test it to see how it performs. Currently our model takes input of the form `review_length, review[500]` where `review[500]` is a sequence of `500` integers which describe the words present in the review, encoded using `word_dict`. Fortunately for us, SageMaker provides built-in inference code for models with simple inputs such as this.\n",
    "\n",
    "There is one thing that we need to provide, however, and that is a function which loads the saved model. This function must be called `model_fn()` and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file which we specified as the entry point. In our case the model loading function has been provided and so no changes need to be made.\n",
    "\n",
    "**NOTE**: When the built-in inference code is run it must import the `model_fn()` method from the `train.py` file. This is why the training code is wrapped in a main guard ( ie, `if __name__ == '__main__':` )\n",
    "\n",
    "Since we don't need to change anything in the code that was uploaded during training, we can simply deploy the current model as-is.\n",
    "\n",
    "**NOTE:** When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until *you* shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for.\n",
    "\n",
    "In other words **If you are no longer using a deployed endpoint, shut it down!**\n",
    "\n",
    "**TODO:** Deploy the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint sagemaker-pytorch-2021-02-28-19-53-25-732: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f231994ac9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Deploy the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                 \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m             )\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2423\u001b[0m         )\n\u001b[1;32m   2424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2425\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   2692\u001b[0m                 ),\n\u001b[1;32m   2693\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InService\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2694\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2695\u001b[0m             )\n\u001b[1;32m   2696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint sagemaker-pytorch-2021-02-28-19-53-25-732: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "# TODO: Deploy the trained model\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Of course, just like in the XGBoost notebook, once we've deployed an endpoint it continues to run until we tell it to shut down. Since we are done using our endpoint for now, we can delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
